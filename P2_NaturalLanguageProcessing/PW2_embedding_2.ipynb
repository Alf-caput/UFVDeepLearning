{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c968420",
   "metadata": {},
   "source": [
    "# Practica 2 - Natural Language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8f458",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b54bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras import Model, Input, layers\n",
    "from tensorflow.keras.layers import Embedding, Dot, Reshape, Dense\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7982c",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f57b9d",
   "metadata": {},
   "source": [
    "## Descarga y proceso de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4e58f",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d34a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallo con encoding utf-8, probando siguiente...\n",
      "Cargado 'train.csv' con encoding latin1\n",
      "Cargado 'test.csv' con encoding latin1\n",
      "Train shape: (27481, 10)\n",
      "Test  shape: (4815, 9)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Directorio portable a P2_NaturalLanguageProcessing/data\n",
    "cwd      = os.getcwd()\n",
    "data_dir = os.path.join(cwd, 'data')\n",
    "\n",
    "# 2. Rutas a los CSV\n",
    "train_path = os.path.join(data_dir, 'train.csv')\n",
    "test_path  = os.path.join(data_dir, 'test.csv')\n",
    "\n",
    "# 3. Lista de codificaciones a probar\n",
    "encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "\n",
    "def load_csv(path, enc_list):\n",
    "    for enc in enc_list:\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=enc)\n",
    "            print(f\"Cargado '{os.path.basename(path)}' con encoding {enc}\")\n",
    "            return df, enc\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Fallo con encoding {enc}, probando siguiente...\")\n",
    "    raise ValueError(f\"No se pudo decodificar {path} con las codificaciones {enc_list}\")\n",
    "\n",
    "# 4. Cargar ambos conjuntos usando la misma codificación\n",
    "train_df, used_enc = load_csv(train_path, encodings)\n",
    "test_df, _      = load_csv(test_path, [used_enc])\n",
    "\n",
    "# 5. Verificación\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test  shape: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f1bd05",
   "metadata": {},
   "source": [
    "### Exploración de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a5800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32296 entries, 0 to 32295\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   textID            31015 non-null  object \n",
      " 1   text              31014 non-null  object \n",
      " 2   selected_text     27480 non-null  object \n",
      " 3   sentiment         31015 non-null  object \n",
      " 4   Time of Tweet     31015 non-null  object \n",
      " 5   Age of User       31015 non-null  object \n",
      " 6   Country           31015 non-null  object \n",
      " 7   Population -2020  31015 non-null  float64\n",
      " 8   Land Area (Km²)   31015 non-null  float64\n",
      " 9   Density (P/Km²)   31015 non-null  float64\n",
      "dtypes: float64(3), object(7)\n",
      "memory usage: 2.5+ MB\n",
      "None\n",
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                         selected_text sentiment Time of Tweet Age of User  \\\n",
      "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
      "1                             Sooo SAD  negative          noon       21-30   \n",
      "2                          bullying me  negative         night       31-45   \n",
      "3                       leave me alone  negative       morning       46-60   \n",
      "4                        Sons of ****,  negative          noon       60-70   \n",
      "\n",
      "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
      "0  Afghanistan        38928346.0         652860.0             60.0  \n",
      "1      Albania         2877797.0          27400.0            105.0  \n",
      "2      Algeria        43851044.0        2381740.0             18.0  \n",
      "3      Andorra           77265.0            470.0            164.0  \n",
      "4       Angola        32866272.0        1246700.0             26.0  \n",
      "Total de muestras: 32296\n",
      "Palabras únicas encontradas: 28490\n",
      "Top 10 palabras más frecuentes: [('i', 18870), ('to', 11249), ('the', 10200), ('a', 7609), ('my', 6264), ('it', 6123), ('you', 6040), ('and', 5827), ('is', 4490), ('in', 4279)]\n",
      "BUFFER_SIZE = 32768\n",
      "BATCH_SIZE = 128\n",
      "VOCAB_SIZE = 8192\n"
     ]
    }
   ],
   "source": [
    "# 5. Juntar los DataFrames para facilitar el preprocesado\n",
    "data_df = pd.concat([train_df, test_df], ignore_index=True)      # concatenación[2]\n",
    "\n",
    "# 6. Inspeccionar estructura básica\n",
    "print(data_df.info())\n",
    "print(data_df.head())\n",
    "\n",
    "# 7. Contar palabras únicas en la columna 'text' para estimar VOCAB_SIZE\n",
    "all_text = ' '.join(data_df['text'].astype(str)).lower()\n",
    "words = re.findall(r'\\b\\w+\\b', all_text)\n",
    "word_counts = Counter(words)\n",
    "unique_words = len(word_counts)\n",
    "\n",
    "print(f\"Total de muestras: {data_df.shape[0]}\")\n",
    "print(f\"Palabras únicas encontradas: {unique_words}\")\n",
    "print(\"Top 10 palabras más frecuentes:\", word_counts.most_common(10))\n",
    "\n",
    "# 8. Definir parámetros para TensorFlow/Keras\n",
    "BUFFER_SIZE = 32768      # chivo mayor al dataset para buen shuffle 2^15\n",
    "BATCH_SIZE = 128        # potencia de 2 adecuada para GPU\n",
    "VOCAB_SIZE = 8192      # tamaño del vocabulario[4]\n",
    "                 \n",
    "print(\"BUFFER_SIZE =\", BUFFER_SIZE)\n",
    "print(\"BATCH_SIZE =\", BATCH_SIZE)\n",
    "print(\"VOCAB_SIZE =\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aad5dd",
   "metadata": {},
   "source": [
    "### Proceso de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88eb256",
   "metadata": {},
   "source": [
    "#### Creación del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2350be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 5 textos filtrados: ['i d have responded if i were going', 'sooo sad i will miss you here in san diego', 'my boss is me', 'what interview leave me alone', 'sons of why couldn t they put them on the releases we already bought']\n",
      "Primeras 5 secuencias: [[1, 163, 19, 7648, 71, 1, 151, 49], [421, 117, 1, 63, 94, 7, 91, 10, 1447, 2230], [5, 1410, 9, 16], [51, 1193, 350, 16, 495], [4254, 13, 118, 472, 14, 72, 332, 131, 17, 3, 7649, 50, 210, 569]]\n",
      "Tamaño de vocabulario efectivo: 8152\n"
     ]
    }
   ],
   "source": [
    "# 1. Seleccionar las 4 096 palabras más frecuentes\n",
    "most_common_words = {w for w, _ in word_counts.most_common(VOCAB_SIZE)}\n",
    "\n",
    "# 2. Filtrar cada texto para quedarnos solo con tokens en el top 4 096\n",
    "corpus_filtered = []\n",
    "for text in data_df['text'].dropna().astype(str):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    filtered_tokens = [t for t in tokens if t in most_common_words]\n",
    "    corpus_filtered.append(\" \".join(filtered_tokens))\n",
    "\n",
    "# 3. Tokenizar el corpus filtrado\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(corpus_filtered)\n",
    "\n",
    "# 4. Convertir textos a secuencias de índices\n",
    "sequences = tokenizer.texts_to_sequences(corpus_filtered)\n",
    "\n",
    "# 5. Diccionarios de mapeo y tamaño final de vocabulario\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = min(len(word2idx) + 1, VOCAB_SIZE)\n",
    "\n",
    "# 6. Mostrar resultados de prueba\n",
    "print(\"Primeros 5 textos filtrados:\", corpus_filtered[:5])\n",
    "print(\"Primeras 5 secuencias:\", sequences[:5])\n",
    "print(\"Tamaño de vocabulario efectivo:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7606597",
   "metadata": {},
   "source": [
    "#### Generación de los pares de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c56d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros pares (t→c): [(np.int32(71), np.int32(151)), (np.int32(7648), np.int32(1)), (np.int32(7648), np.int32(163)), (np.int32(49), np.int32(151)), (np.int32(1), np.int32(151)), (np.int32(19), np.int32(163)), (np.int32(19), np.int32(1)), (np.int32(7648), np.int32(19)), (np.int32(71), np.int32(1)), (np.int32(151), np.int32(1))]\n",
      "Total pares: 1384586\n"
     ]
    }
   ],
   "source": [
    "# Parámetros\n",
    "window_size      = 2\n",
    "negative_samples = 0.0\n",
    "seed_value       = 42               # cualquier entero\n",
    "\n",
    "pairs = []\n",
    "for seq in sequences:\n",
    "    if len(seq) < 2:\n",
    "        continue\n",
    "\n",
    "    # pasar seed para evitar el randint interno con float\n",
    "    sg_pairs, _ = skipgrams(\n",
    "        sequence=seq,\n",
    "        vocabulary_size=vocab_size,\n",
    "        window_size=window_size,\n",
    "        negative_samples=negative_samples,\n",
    "        shuffle=True,\n",
    "        seed=seed_value\n",
    "    )\n",
    "    pairs.extend(sg_pairs)\n",
    "\n",
    "# separar en arrays\n",
    "if pairs:\n",
    "    targets, contexts = zip(*pairs)\n",
    "    targets  = np.array(targets,  dtype='int32')\n",
    "    contexts = np.array(contexts, dtype='int32')\n",
    "else:\n",
    "    targets  = np.zeros((0,), dtype='int32')\n",
    "    contexts = np.zeros((0,), dtype='int32')\n",
    "\n",
    "print(\"Primeros pares (t→c):\", list(zip(targets[:10], contexts[:10])))\n",
    "print(\"Total pares:\", len(pairs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
